Parse re-ranker (HW 1)
- My final perplexity score on the validation data = 9.7
- My hyperparameters:
    - HIDDEN_SIZE = 1500
    - EMBEDDING_SIZE = 100
    - BATCH_SIZE = 20
    - WINDOW_LENGTH = 50
    - RNN_LAYERS = 1
    - NUM_EPOCHS = 1
- I tried training for more epochs (5), but that resulted in overfitting (dev loss and dev perplexity went up while the training loss + perplexity went down)
- In order to mitigate overfitting while training for more epochs, I tried adding dropout (with probability 0.5) to the embedding layer. While this helped the perplexity score was 10.46)
- I think that in order to train for more than 1 epoch and not overfit, I would have to shuffle the sentences in between epochs so as to prevent the model from memorizing the training data.

